import json
from llm_rag_qa_documents.factory.chroma_factory import ChromadbFactory
from llm_rag_qa_documents.prompts.prompts_config import promptsConfig
from llm_rag_qa_documents.factory.watsonx_factory import WatsonXFactory
from llm_rag_qa_documents.factory.logger_factory import loggerFactory

class llmHelper():
    def __init__(self):
        self.chromadb = ChromadbFactory("llm_rag_qa_documents/data/chromadb")
        self.prompts = promptsConfig()
        self.llm = WatsonXFactory()
        self.logger = loggerFactory().get_logger(__name__)
        """
        Initialize the LLM helper class by instanciating all the required class to be used across its methods.
        """

    def format_chroma_results(self, id_list, doc_list, metadata_list, distance_list):
        """
        Get the required lists generated by chroma query collection and brings them together with all parameters from a row are in the same sublist.

        :param id_list: a list of chromadb collection ids
        :type id_list: List
        :param doc_list: a list of chromadb collection documents (the actual row value)
        :type doc_list: List
        :param metadata_list: a list of chromadb collection metadata
        :type metadata_list: List
        :param distance_list: a list of chromadb collection numeric distance
        :type distance_list: List
        """
        l = []
        for i in zip(id_list, doc_list, metadata_list, distance_list):
            l.append(list(i))
        return l
    
    def generate_docs_context(self, question):
        """
        Create the context (piece of the prompt) with the relevant chunks of information from the provided documents based on the question asked.

        :param question: a user question about something
        :type question: String
        """  
        objects_retrieved = self.chromadb.collection_query(question, 4, "docs_collection")

        print(objects_retrieved)

        objects_formatted = self.format_chroma_results(objects_retrieved["ids"][0], objects_retrieved["documents"][0], objects_retrieved["metadatas"][0], objects_retrieved["distances"][0])

        context = "\n "
        for i in objects_formatted:
            context = "".join([context, f"{i[1]} \n"])

        return context
       
    def talkToMe(self, question):
        """
        Brings all the contexts and prompts together based on the questions and provide it all to the llm to get an answer.

        :param question: a user question about something
        :type question: String
        """  

        llm_answer = self.generate_docs_context(question)
        prompt_final = {"input":question, "context": llm_answer}
        prompt_complete = self.prompts.get_prompt_ready("docs", prompt_final)
        
        self.logger.info(f"Prompt Docs: {prompt_complete}")
        answer = ""
        for chunk in self.llm.ask_llm_stream(prompt_complete):
            answer += chunk
            yield chunk
        self.logger.info(f"Answer: {answer}")